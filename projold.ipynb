{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "import tensorflow_addons as tfa\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import *\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from hp import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "num_classes = 100\r\n",
    "input_shape = (32, 32, 3)\r\n",
    "checkpoint_path = \"C:\\\\Users\\\\tom7l\\\\ckpoint\"\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\r\n",
    "\r\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\r\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cropped_image = tf.image.crop_to_bounding_box(x_train[0], 0, 0, 31, 32)\r\n",
    "padded_image = tf.image.pad_to_bounding_box(cropped_image, 0, 0, 32, 32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "num_classes = 100\r\n",
    "input_shape = (32, 32, 3)\r\n",
    "\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\r\n",
    "a=tf.keras.preprocessing.image.random_shift(\r\n",
    "    x_train[0],1, 0, row_axis=1, col_axis=2, channel_axis=0,\r\n",
    "    fill_mode='nearest', cval=0.0, interpolation_order=1\r\n",
    ")\r\n",
    "a=tf.keras.preprocessing.image.random_shift(\r\n",
    "    x_train[0],15, 0, row_axis=1, col_axis=2, channel_axis=0,\r\n",
    "    fill_mode='nearest', cval=0.0, interpolation_order=1\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\r\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_augmentation = keras.Sequential(\r\n",
    "    [\r\n",
    "        Normalization(),\r\n",
    "        Resizing(image_size, image_size),\r\n",
    "        RandomFlip(\"horizontal\"),\r\n",
    "        RandomZoom(height_factor=0.2, width_factor=0.2),\r\n",
    "    ],\r\n",
    "    name=\"data_augmentation\",\r\n",
    ")\r\n",
    "# Compute the mean and the variance of the training data for normalization.\r\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\r\n",
    "def build_classifier(blocks, positional_encoding=False):\r\n",
    "    inputs = layers.Input(shape=input_shape)\r\n",
    "    \r\n",
    "\r\n",
    "    # Augment data.\r\n",
    "    augmented = data_augmentation(inputs)\r\n",
    "    \r\n",
    "    # Create patches.\r\n",
    "    patches = Patches(patch_size, num_patches)(augmented)\r\n",
    "    \r\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\r\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\r\n",
    "    if positional_encoding:\r\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\r\n",
    "        position_embedding = layers.Embedding(\r\n",
    "            input_dim=num_patches, output_dim=embedding_dim\r\n",
    "        )(positions)\r\n",
    "        x = x + position_embedding\r\n",
    "    # Process x using the module blocks.\r\n",
    "    x = blocks(x)\r\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\r\n",
    "    representation = layers.GlobalAveragePooling1D()(x)\r\n",
    "    # Apply dropout.\r\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\r\n",
    "    # Compute logits outputs.\r\n",
    "    logits = layers.Dense(num_classes)(representation)\r\n",
    "    # Create the Keras model.\r\n",
    "    return keras.Model(inputs=inputs, outputs=logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "  \r\n",
    "def run_experiment(model):\r\n",
    "    # Create Adam optimizer with weight decay.\r\n",
    "    optimizer = tfa.optimizers.AdamW(\r\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\r\n",
    "    )\r\n",
    "    # Compile the model.\r\n",
    "    model.compile(\r\n",
    "        optimizer=optimizer,\r\n",
    "\r\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "        metrics=[\r\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\r\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\r\n",
    "        ],\r\n",
    "    )\r\n",
    "    # Create a learning rate scheduler callback.\r\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\r\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\r\n",
    "    )\r\n",
    "    # Create an early stopping callback.\r\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\r\n",
    "    )\r\n",
    "    # Fit the model.\r\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n",
    "                                                 save_weights_only=True,\r\n",
    "                                                 verbose=1)\r\n",
    "    history = model.fit(\r\n",
    "        x=x_train,\r\n",
    "        y=y_train,\r\n",
    "        batch_size=batch_size,\r\n",
    "        epochs=num_epochs,\r\n",
    "        validation_split=0.1,\r\n",
    "        callbacks=[early_stopping, reduce_lr,cp_callback],\r\n",
    "    )\r\n",
    "\r\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\r\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\r\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\r\n",
    "\r\n",
    "    # Return history to plot learning curves.\r\n",
    "    return history\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from classes import MLPMixerLayer, Patches\r\n",
    "mlpmixer_blocks = keras.Sequential(\r\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\r\n",
    ")\r\n",
    "\r\n",
    "model = build_classifier(mlpmixer_blocks)\r\n",
    "\r\n",
    "# history = run_experiment(mlpmixer_classifier)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tom7l\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:376: UserWarning: Default value of `approximate` is changed from `True` to `False`\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# load trained model\r\n",
    "\r\n",
    "model.load_weights(checkpoint_path)\r\n",
    "# Create Adam optimizer with weight decay.\r\n",
    "optimizer = tfa.optimizers.AdamW(\r\n",
    "    learning_rate=learning_rate, weight_decay=weight_decay,\r\n",
    ")\r\n",
    "# Compile the model.\r\n",
    "model.compile(\r\n",
    "    optimizer=optimizer,\r\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=[\r\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\r\n",
    "        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\r\n",
    "    ],\r\n",
    ")\r\n",
    "loss, acc, top5_acc = model.evaluate(x_test, y_test, verbose=2)\r\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 - 17s - loss: 1.8927 - acc: 0.5058 - top5-acc: 0.7952\n",
      "Restored model, accuracy: 50.58%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from patch_test import *\r\n",
    "\r\n",
    "# img = x_test[10]\r\n",
    "# # img=crop_with_direction(img,bottom=1)\r\n",
    "# # img=np.array(img)\r\n",
    "# img=img.reshape((1,32,32,3))\r\n",
    "# y_test[0].reshape((1,1))\r\n",
    "# a=np.argmax(model.predict(img))\r\n",
    "# predict_mat = model.predict(img)[0]\r\n",
    "# # print(predict_mat)\r\n",
    "# print(max(predict_mat))\r\n",
    "# second_max_idx =  np.argpartition(predict_mat.flatten(), -2)[-2:]\r\n",
    "# second_max_value = predict_mat[second_max_idx][0]\r\n",
    "# print(second_max_value)\r\n",
    "# # model.evaluate(img.reshape((1,32,32,3)),y_test[15],verbose=2)\r\n",
    "\r\n",
    "def find_smallest_diffs(k):\r\n",
    "    slice_object = slice(50)\r\n",
    "    # diffs = [None] * x_test.size\r\n",
    "    diffs = [None] * 50\r\n",
    "    for i, image in enumerate(x_test[slice_object]):\r\n",
    "        image = image.reshape((1,32,32,3))\r\n",
    "        # max=np.argmax(model.predict(image))\r\n",
    "        predict_matrix = model.predict(image)[0]\r\n",
    "        max_value = max(predict_matrix)\r\n",
    "        second_max_idx =  np.argpartition(predict_matrix.flatten(), -2)[-2:]\r\n",
    "        second_max_value = predict_matrix[second_max_idx][0]\r\n",
    "        diffs[i] = max_value-second_max_value\r\n",
    "    print(diffs)\r\n",
    "    slice_k = slice(k)\r\n",
    "    smallest_images = np.argpartition(diffs,k)[slice_k]\r\n",
    "    print(smallest_images)\r\n",
    "\r\n",
    "find_smallest_diffs(5)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6493306, 2.6064634, 0.94237137, 0.051816463, 3.9593754, 2.925706, 1.1266809, 0.4871168, 4.259143, 0.876956, 1.2505922, 3.5374303, 0.25746918, 7.3994026, 0.79383945, 2.7927084, 2.7273493, 0.34855556, 1.6368322, 1.5857592, 1.0949249, 1.2154498, 0.18168926, 0.5239978, 6.6065607, 2.1589203, 6.3039017, 4.675338, 1.2184777, 1.3687944, 3.8053799, 2.9740095, 2.4551196, 0.12508774, 2.702506, 0.76242495, 2.1097994, 0.82581806, 1.9268599, 0.31149578, 1.6002016, 0.096577644, 3.860211, 1.8792591, 7.0126657, 1.6900587, 0.81520367, 0.72397614, 4.0528383, 1.192544]\n",
      "[ 3 41 33 22 12]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('ai': conda)"
  },
  "interpreter": {
   "hash": "a998e209630e24575f17437a900cf41a78662deba6fa748cb0a8ac9bf2b62a4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}