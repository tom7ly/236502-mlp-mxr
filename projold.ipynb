{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "import tensorflow_addons as tfa\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import *\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from hp import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tom7l\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "num_classes = 100\r\n",
    "input_shape = (32, 32, 3)\r\n",
    "checkpoint_path = \"C:\\\\Users\\\\tom7l\\\\ckpoint\"\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\r\n",
    "\r\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\r\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "cropped_image = tf.image.crop_to_bounding_box(x_train[0], 0, 0, 31, 32)\r\n",
    "padded_image = tf.image.pad_to_bounding_box(cropped_image, 0, 0, 32, 32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "num_classes = 100\r\n",
    "input_shape = (32, 32, 3)\r\n",
    "\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\r\n",
    "a=tf.keras.preprocessing.image.random_shift(\r\n",
    "    x_train[0],1, 0, row_axis=1, col_axis=2, channel_axis=0,\r\n",
    "    fill_mode='nearest', cval=0.0, interpolation_order=1\r\n",
    ")\r\n",
    "a=tf.keras.preprocessing.image.random_shift(\r\n",
    "    x_train[0],15, 0, row_axis=1, col_axis=2, channel_axis=0,\r\n",
    "    fill_mode='nearest', cval=0.0, interpolation_order=1\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\r\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_augmentation = keras.Sequential(\r\n",
    "    [\r\n",
    "        Normalization(),\r\n",
    "        Resizing(image_size, image_size),\r\n",
    "        RandomFlip(\"horizontal\"),\r\n",
    "        RandomZoom(height_factor=0.2, width_factor=0.2),\r\n",
    "    ],\r\n",
    "    name=\"data_augmentation\",\r\n",
    ")\r\n",
    "# Compute the mean and the variance of the training data for normalization.\r\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "def build_classifier(blocks, positional_encoding=False):\r\n",
    "    inputs = layers.Input(shape=input_shape)\r\n",
    "    \r\n",
    "\r\n",
    "    # Augment data.\r\n",
    "    augmented = data_augmentation(inputs)\r\n",
    "    \r\n",
    "    # Create patches.\r\n",
    "    patches = Patches(patch_size, num_patches)(augmented)\r\n",
    "    \r\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\r\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\r\n",
    "    if positional_encoding:\r\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\r\n",
    "        position_embedding = layers.Embedding(\r\n",
    "            input_dim=num_patches, output_dim=embedding_dim\r\n",
    "        )(positions)\r\n",
    "        x = x + position_embedding\r\n",
    "    # Process x using the module blocks.\r\n",
    "    x = blocks(x)\r\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\r\n",
    "    representation = layers.GlobalAveragePooling1D()(x)\r\n",
    "    # Apply dropout.\r\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\r\n",
    "    # Compute logits outputs.\r\n",
    "    logits = layers.Dense(num_classes)(representation)\r\n",
    "    # Create the Keras model.\r\n",
    "    return keras.Model(inputs=inputs, outputs=logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "  \r\n",
    "def run_experiment(model):\r\n",
    "    # Create Adam optimizer with weight decay.\r\n",
    "    optimizer = tfa.optimizers.AdamW(\r\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay,\r\n",
    "    )\r\n",
    "    # Compile the model.\r\n",
    "    model.compile(\r\n",
    "        optimizer=optimizer,\r\n",
    "\r\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "        metrics=[\r\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\r\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\r\n",
    "        ],\r\n",
    "    )\r\n",
    "    # Create a learning rate scheduler callback.\r\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\r\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5\r\n",
    "    )\r\n",
    "    # Create an early stopping callback.\r\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\r\n",
    "    )\r\n",
    "    # Fit the model.\r\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n",
    "                                                 save_weights_only=True,\r\n",
    "                                                 verbose=1)\r\n",
    "    history = model.fit(\r\n",
    "        x=x_train,\r\n",
    "        y=y_train,\r\n",
    "        batch_size=batch_size,\r\n",
    "        epochs=num_epochs,\r\n",
    "        validation_split=0.1,\r\n",
    "        callbacks=[early_stopping, reduce_lr,cp_callback],\r\n",
    "    )\r\n",
    "\r\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\r\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\r\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\r\n",
    "\r\n",
    "    # Return history to plot learning curves.\r\n",
    "    return history\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from classes import MLPMixerLayer, Patches\r\n",
    "mlpmixer_blocks = keras.Sequential(\r\n",
    "    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\r\n",
    ")\r\n",
    "\r\n",
    "model = build_classifier(mlpmixer_blocks)\r\n",
    "\r\n",
    "# history = run_experiment(mlpmixer_classifier)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tom7l\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:376: UserWarning: Default value of `approximate` is changed from `True` to `False`\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# load trained model\r\n",
    "\r\n",
    "model.load_weights(checkpoint_path)\r\n",
    "# Create Adam optimizer with weight decay.\r\n",
    "optimizer = tfa.optimizers.AdamW(\r\n",
    "    learning_rate=learning_rate, weight_decay=weight_decay,\r\n",
    ")\r\n",
    "# Compile the model.\r\n",
    "model.compile(\r\n",
    "    optimizer=optimizer,\r\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=[\r\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\r\n",
    "        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\r\n",
    "    ],\r\n",
    ")\r\n",
    "loss, acc, top5_acc = model.evaluate(x_test, y_test, verbose=2)\r\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 - 18s - loss: 1.8927 - acc: 0.5058 - top5-acc: 0.7952\n",
      "Restored model, accuracy: 50.58%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from patch_test import *\r\n",
    "\r\n",
    "# img = x_test[10]\r\n",
    "# # img=crop_with_direction(img,bottom=1)\r\n",
    "# # img=np.array(img)\r\n",
    "# img=img.reshape((1,32,32,3))\r\n",
    "# y_test[0].reshape((1,1))\r\n",
    "# a=np.argmax(model.predict(img))\r\n",
    "# predict_mat = model.predict(img)[0]\r\n",
    "# # print(predict_mat)\r\n",
    "# print(max(predict_mat))\r\n",
    "# second_max_idx =  np.argpartition(predict_mat.flatten(), -2)[-2:]\r\n",
    "# second_max_value = predict_mat[second_max_idx][0]\r\n",
    "# print(second_max_value)\r\n",
    "# # model.evaluate(img.reshape((1,32,32,3)),y_test[15],verbose=2)\r\n",
    "\r\n",
    "# print(x_test.shape)\r\n",
    "# def find_smallest_diffs(k):\r\n",
    "#     slice_object = slice(50)\r\n",
    "#     # diffs = [None] * x_test.size\r\n",
    "#     diffs = [None] * 50\r\n",
    "#     for i, image in enumerate(x_test[slice_object]):\r\n",
    "#         image = image.reshape((1,32,32,3))\r\n",
    "#         # max=np.argmax(model.predict(image))\r\n",
    "#         predict_matrix = model.predict(image)[0]\r\n",
    "#         max_value = max(predict_matrix)\r\n",
    "#         second_max_idx =  np.argpartition(predict_matrix.flatten(), -2)[-2:]\r\n",
    "#         second_max_value = predict_matrix[second_max_idx][0]\r\n",
    "#         diffs[i] = max_value-second_max_value\r\n",
    "#     print(diffs)\r\n",
    "#     slice_k = slice(k)\r\n",
    "#     smallest_images = np.argpartition(diffs,k)[slice_k]\r\n",
    "#     print(smallest_images)\r\n",
    "\r\n",
    "# find_smallest_diffs(5)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from patch_test import *\r\n",
    "\r\n",
    "def find_smallest_diffs_by_threshold(t):\r\n",
    "    slice_object = slice(200)\r\n",
    "    # diffs = [None] * 200\r\n",
    "    diffs = [None] * (x_test.size)\r\n",
    "    below_threshold_images = []\r\n",
    "    # for i, image in enumerate(x_test[slice_object]):\r\n",
    "    for i, image in enumerate(x_test):\r\n",
    "        image = image.reshape((1,32,32,3))\r\n",
    "        predict_matrix = model.predict(image)[0]\r\n",
    "        max_value = max(predict_matrix)\r\n",
    "        second_max_idx =  np.argpartition(predict_matrix.flatten(), -2)[-2:]\r\n",
    "        second_max_value = predict_matrix[second_max_idx][0]\r\n",
    "        diffs[i] = max_value-second_max_value\r\n",
    "        if(diffs[i] <= t):\r\n",
    "            below_threshold_images.append(i)\r\n",
    "    # print(diffs)\r\n",
    "    # print(below_threshold_images)\r\n",
    "    print(len(below_threshold_images))\r\n",
    "find_smallest_diffs_by_threshold(0.05)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from patch_test import *\r\n",
    "img = x_test[15]\r\n",
    "# cropped_img = crop_with_direction(img, top=1)\r\n",
    "plt.imshow(img)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# m = nn.Softmax(dim=1)\r\n",
    "# image = m(x_test[10])\r\n",
    "# image=image.reshape((1,32,32,3))\r\n",
    "# print(model.predict(image))\r\n",
    "\r\n",
    "img = x_test[15]\r\n",
    "img=img.reshape((1,32,32,3))\r\n",
    "print(model.predict(img))\r\n",
    "orig_predict = np.argmax(model.predict(img))\r\n",
    "print(\"original predict: {}\".format(orig_predict))\r\n",
    "# img=crop_with_direction(img,left=1)\r\n",
    "# # img=np.array(img)\r\n",
    "# img=img.reshape((1,32,32,3))\r\n",
    "# # y_test[0].reshape((1,1))\r\n",
    "# # a=np.argmax(model.predict(img))\r\n",
    "# predict_mat = model.predict(img)[0]\r\n",
    "# # print(predict_mat)\r\n",
    "# print(max(predict_mat))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('ai': conda)"
  },
  "interpreter": {
   "hash": "a998e209630e24575f17437a900cf41a78662deba6fa748cb0a8ac9bf2b62a4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}